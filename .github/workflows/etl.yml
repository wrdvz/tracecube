name: ETL to R2

on:
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * 1'   # lundi 02:00 UTC

jobs:
  etl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: |
          python -V
          pip install -U pip wheel
          pip install -r etl/requirements.txt awscli

      - name: Run ETL (produce files in data/out)
        run: |
          set -euxo pipefail
          mkdir -p data/out
          python -u etl/run_etl.py
          ls -la data/out || true

      - name: Upload to R2 (if any file exists)
        env:
          R2_ENDPOINT: https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: auto
          AWS_S3_FORCE_PATH_STYLE: "true"     # ðŸ‘ˆ AJOUT
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
       run: |
         set -euxo pipefail
         shopt -s nullglob
         FILES=(data/out/*)
         if [ ${#FILES[@]} -eq 0 ]; then
         echo "No files in data/out â€” skipping upload."; exit 0; fi
         TS=$(date -u +"%Y%m%d-%H%M%S")
         aws --endpoint-url "$R2_ENDPOINT" s3 cp data/out/ "s3://$R2_BUCKET/v/$TS/" --recursive
         aws --endpoint-url "$R2_ENDPOINT" s3 rm "s3://$R2_BUCKET/latest/" --recursive || true
         aws --endpoint-url "$R2_ENDPOINT" s3 cp "s3://$R2_BUCKET/v/$TS/" "s3://$R2_BUCKET/latest/" --recursive
