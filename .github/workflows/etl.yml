name: ETL to R2

on:
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * 1'   # lundi 02:00 UTC

jobs:
  etl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: |
          python -V
          pip install -U pip wheel
          pip install -r etl/requirements.txt awscli boto3

      - name: Run ETL (produce files in data/out)
        run: |
          set -euxo pipefail
          mkdir -p data/out
          python -u etl/run_etl.py
          ls -la data/out || true

      - name: Upload to R2 (if any file exists, via boto3)
        env:
          R2_ENDPOINT: https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          set -euxo pipefail
          python - << 'PY'
          import os, sys, time, pathlib
          import boto3
          from botocore.client import Config

          raw = os.environ["R2_ENDPOINT"].strip()
          endpoint = raw if raw.startswith(("http://", "https://")) else f"https://{raw}.r2.cloudflarestorage.com"
          bucket   = os.environ["R2_BUCKET"]
          akid     = os.environ["AWS_ACCESS_KEY_ID"]
          secret   = os.environ["AWS_SECRET_ACCESS_KEY"]

          s3 = boto3.client(
              "s3",
              endpoint_url=endpoint,
              aws_access_key_id=akid,
              aws_secret_access_key=secret,
              region_name="auto",
              config=Config(signature_version="s3v4", s3={"addressing_style": "path"})
          )

          out = pathlib.Path("data/out")
          files = [p for p in out.glob("*") if p.is_file()]
          if not files:
              print("No files in data/out — skipping upload.")
              sys.exit(0)

          ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
          ver_prefix = f"v/{ts}/"
          latest_prefix = "latest/"

          # upload version horodatée
          for p in files:
              s3.upload_file(str(p), bucket, ver_prefix + p.name)

          # refresh latest
          for p in files:
              s3.copy_object(
                  Bucket=bucket,
                  CopySource={"Bucket": bucket, "Key": ver_prefix + p.name},
                  Key=latest_prefix + p.name
              )

          print("Upload complete.")
          PY
